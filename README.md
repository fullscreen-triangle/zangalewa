# Zangalewa

<h1 align="center">Zangalewa</h1>
<p align="center"><em>"I take responsibility for my actions"</em></p>

<p align="center">
  <img src="zangalewa.png" alt="Zangalewa Logo">
</p>

<div align="center">

![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)
![Status: In Development](https://img.shields.io/badge/Status-In%20Development-blue)

</div>

## Table of Contents
- [Overview](#overview)
- [Motivation](#motivation)
- [Core Features](#core-features)
- [System Architecture](#system-architecture)
- [Technology Stack](#technology-stack)
- [Project Structure](#project-structure)
- [Implementation Details](#implementation-details)
  - [Chat Interface](#chat-interface)
  - [Command Execution System](#command-execution-system)
  - [Codebase Analysis System](#codebase-analysis-system)
  - [Knowledge Base Construction](#knowledge-base-construction)
  - [Metacognitive Layer](#metacognitive-layer)
  - [Error Handling System](#error-handling-system)
  - [Visual Presentation Layer](#visual-presentation-layer)
- [Installation and Setup](#installation-and-setup)
- [Configuration](#configuration)
- [Getting Started with HuggingFace API](#getting-started-with-huggingface-api)
- [Usage Examples](#usage-examples)
- [Development Roadmap](#development-roadmap)
- [Completed Improvements](#completed-improvements)
- [Contributing](#contributing)
- [License](#license)
- [LLM Integration](#llm-integration)
  - [HuggingFace API Requirement](#huggingface-api-requirement)
  - [Setting Up HuggingFace API](#setting-up-huggingface-api)
  - [Optional Commercial API Support](#optional-commercial-api-support)
  - [Model Selection](#model-selection)

## Overview

Zangalewa is an AI-powered command-line assistant designed specifically for bioinformatics and other technical fields where command-line interfaces are prevalent. It leverages state-of-the-art language models (OpenAI, Claude) to orchestrate complex workflows, analyze codebases, generate documentation, and provide intelligent error handling - all through a visually enhanced CLI experience.

## Motivation

This project emerged from two primary pain points in technical workflows:

1. **Project Management Complexity**: Managing multiple Python projects with different configurations, dependencies, and execution patterns creates significant cognitive overhead.

2. **Command Line Friction**: Traditional command-line interfaces lack intuitiveness and visual appeal, making them intimidating for users transitioning between operating systems or those who don't regularly use terminal environments.

The command line remains the dominant interface in bioinformatics due to the nature of the tools and workflows. However, many projects:
- Have complex installation procedures
- Require cryptic configuration settings
- Demand understanding of concepts outside the user's primary domain
- Provide poor error feedback
- Lack comprehensive documentation

Zangalewa aims to solve these issues by creating an intelligent layer between the user and the command line, making complex technical workflows more accessible while maintaining the power and flexibility of CLI tools.

## Core Features

Zangalewa offers the following key capabilities:

1. **Intelligent Project Management**
   - GitHub repository fetching and automatic installation
   - Dependency resolution and environment setup
   - Configuration management based on system specifications

2. **Automated Documentation**
   - Comprehensive codebase analysis
   - Function-level documentation generation
   - Creation of searchable knowledge base for future reference

3. **AI-Powered Workflow Construction**
   - Natural language-based task description
   - Automated pipeline generation optimized for time/memory constraints
   - System-aware configuration recommendations

4. **Enhanced CLI Experience**
   - Visually rich terminal interface with modern design elements
   - Informative progress indicators and data visualizations
   - More intuitive command structure

5. **Sophisticated Error Resolution**
   - Multi-source error analysis (local knowledge + web search)
   - Contextual error explanation
   - Automated error correction recommendations

6. **Metacognitive Orchestration**
   - Process monitoring and optimization
   - Context tracking across sessions
   - Learning from user interactions to improve suggestions

## System Architecture

The Zangalewa system consists of several interconnected components:

```
┌───────────────────────────────────────────────────────────────┐
│                        User Interface                         │
│                                                               │
│  ┌─────────────────┐  ┌──────────────────┐  ┌───────────────┐ │
│  │ Command Prompt  │  │  Visual Display  │  │ Input Parser  │ │
│  └─────────────────┘  └──────────────────┘  └───────────────┘ │
└───────────────────────────────┬───────────────────────────────┘
                                │
┌───────────────────────────────▼───────────────────────────────┐
│                      Metacognitive Layer                      │
│                                                               │
│  ┌─────────────────┐  ┌──────────────────┐  ┌───────────────┐ │
│  │ Process Manager │  │  Context Tracker │  │ Learning Unit │ │
│  └─────────────────┘  └──────────────────┘  └───────────────┘ │
└───────────────────────────────┬───────────────────────────────┘
                                │
┌───────────────────────────────▼───────────────────────────────┐
│                       Core Service Layer                      │
│                                                               │
│  ┌─────────────────┐  ┌──────────────────┐  ┌───────────────┐ │
│  │  LLM Interface  │  │ Command Executor │  │ Error Handler │ │
│  └─────────────────┘  └──────────────────┘  └───────────────┘ │
│                                                               │
│  ┌─────────────────┐  ┌──────────────────┐  ┌───────────────┐ │
│  │  Code Analyzer  │  │ Knowledge Base   │  │ Web Searcher  │ │
│  └─────────────────┘  └──────────────────┘  └───────────────┘ │
└───────────────────────────────┬───────────────────────────────┘
                                │
┌───────────────────────────────▼───────────────────────────────┐
│                         System Layer                          │
│                                                               │
│  ┌─────────────────┐  ┌──────────────────┐  ┌───────────────┐ │
│  │   File System   │  │  Network Access  │  │ Process Mgmt  │ │
│  └─────────────────┘  └──────────────────┘  └───────────────┘ │
└───────────────────────────────────────────────────────────────┘
```

## Technology Stack

Zangalewa will be built using the following technologies:

- **Core Language**: Python 3.10+
- **Terminal Interface**: Rich, Textual
- **AI/ML Integration**: 
  - HuggingFace API (Primary)
  - OpenAI API (Optional)
  - Anthropic Claude API (Optional)
- **Code Analysis**: ast, astroid, pylint, radon
- **Knowledge Base**: SQLite, FAISS
- **Process Management**: subprocess, psutil
- **Networking**: aiohttp, requests
- **Testing**: pytest, hypothesis
- **Documentation**: Sphinx, MkDocs
- **Packaging**: Poetry

## Project Structure

```
Zangalewa/
├── LICENSE
├── README.md
├── pyproject.toml        # Poetry configuration
├── .env.example          # Example environment variables
├── .gitignore
├── Zangalewa/           # Main package
│   ├── __init__.py
│   ├── cli/              # CLI application
│   │   ├── __init__.py
│   │   ├── app.py        # Main application entry point
│   │   ├── commands/     # Command implementations
│   │   └── ui/           # UI components
│   │       ├── styles.py
│   │       ├── widgets.py
│   │       └── screens/  # Screen definitions
│   │
│   ├── core/             # Core functionality
│   │   ├── __init__.py
│   │   ├── llm/          # LLM integration
│   │   │   ├── openai.py
│   │   │   ├── anthropic.py
│   │   │   └── prompts/  # System prompts
│   │   │
│   │   ├── executor/     # Command execution
│   │   │   ├── command.py
│   │   │   └── shell.py
│   │   │
│   │   ├── analyzer/     # Code analysis
│   │   │   ├── parser.py
│   │   │   ├── documenter.py
│   │   │   └── metrics.py
│   │   │
│   │   ├── knowledge/    # Knowledge base
│   │   │   ├── store.py
│   │   │   ├── query.py
│   │   │   └── embeddings.py
│   │   │
│   │   └── errors/       # Error handling
│   │       ├── detector.py
│   │       ├── resolver.py
│   │       └── search.py
│   │
│   ├── meta/             # Metacognitive layer
│   │   ├── __init__.py
│   │   ├── context.py    # Context tracking
│   │   ├── orchestrator.py # Process orchestration
│   │   ├── learning.py   # Learning from interactions
│   │   └── tracker.py    # Session tracking
│   │
│   └── utils/            # Utilities
│       ├── __init__.py
│       ├── system.py     # System information
│       ├── logging.py    # Logging functionality
│       ├── config.py     # Configuration management
│       └── security.py   # API key management
│
├── tests/                # Test suite
│   ├── __init__.py
│   ├── conftest.py
│   ├── test_cli/
│   ├── test_core/
│   └── test_meta/
│
├── docs/                 # Documentation
│   ├── index.md
│   ├── architecture.md
│   ├── usage.md
│   └── development.md
│
└── examples/             # Example scripts and configurations
    ├── config_examples/
    └── workflow_examples/
```

## Implementation Details

### Chat Interface

The chat interface serves as the primary interaction point for users. It's designed to look and behave like a traditional terminal but with enhanced visual elements and AI-powered responses.

**Key Components:**
- Natural language command parsing
- History-aware conversation tracking
- Context-sensitive auto-completion
- Rich text formatting for responses
- Inline syntax highlighting
- Progress indicators and spinners
- Command suggestion system

**Implementation Approach:**
The interface will be built using the Rich and Textual libraries, providing a TUI (Text-based User Interface) that balances traditional terminal aesthetics with modern design elements. The interface will maintain a conversational context that allows the AI to understand references to previous commands and outputs.

```python
# Conceptual interface implementation
class ZangalewaShell:
    def __init__(self):
        self.conversation_history = []
        self.context_manager = ContextManager()
        self.llm_client = LLMClient()
        
    async def process_input(self, user_input: str) -> str:
        # Add user input to conversation history
        self.conversation_history.append({"role": "user", "content": user_input})
        
        # Update context with user input
        self.context_manager.update(user_input)
        
        # Determine if this is a direct command or needs AI processing
        if self._is_direct_command(user_input):
            result = await self._execute_direct_command(user_input)
        else:
            # Process with AI
            result = await self._process_with_ai(user_input)
            
        # Add response to history
        self.conversation_history.append({"role": "assistant", "content": result})
        
        return result
```

### Command Execution System

The command execution system manages the interaction between the AI assistant and the underlying operating system, securely executing commands while monitoring their execution.

**Key Components:**
- Command validation and security filtering
- Execution environment management
- Resource monitoring (CPU, memory, disk, network)
- Output capturing and formatting
- Error detection and handling
- Command timeout and cancellation

**Implementation Approach:**
Commands will be executed in controlled environments with appropriate security boundaries. The system will monitor resource usage and execution time, providing real-time feedback to the user and collecting data for error handling if needed.

```python
# Conceptual command executor
class CommandExecutor:
    def __init__(self):
        self.error_handler = ErrorHandler()
        
    async def execute(self, command: str, environment: dict = None) -> ExecutionResult:
        # Validate command for security
        self._validate_command(command)
        
        # Prepare execution environment
        env = os.environ.copy()
        if environment:
            env.update(environment)
            
        # Start resource monitoring
        monitor = ResourceMonitor.start()
        
        try:
            # Execute command
            process = await asyncio.create_subprocess_shell(
                command,
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE,
                env=env
            )
            
            # Capture output
            stdout, stderr = await process.communicate()
            
            # Check for errors
            if process.returncode != 0:
                error_info = await self.error_handler.analyze(
                    command=command,
                    return_code=process.returncode,
                    stderr=stderr.decode()
                )
                return ExecutionResult(
                    success=False,
                    output=stdout.decode(),
                    error=stderr.decode(),
                    error_analysis=error_info,
                    resources=monitor.stop()
                )
                
            return ExecutionResult(
                success=True,
                output=stdout.decode(),
                resources=monitor.stop()
            )
            
        except Exception as e:
            return ExecutionResult(
                success=False,
                error=str(e),
                resources=monitor.stop()
            )
```

### Codebase Analysis System

The codebase analysis system examines and documents code repositories to provide insights and generate comprehensive documentation.

**Key Components:**
- Multi-language code parsing
- Structure and dependency analysis
- Function and class documentation
- API endpoint identification
- Code quality metrics
- Usage pattern detection
- Test coverage analysis

**Implementation Approach:**
The system will use a combination of AST (Abstract Syntax Tree) parsing, static analysis tools, and AI-powered code understanding to generate comprehensive documentation of codebases. This documentation will be stored in markdown format and indexed for later retrieval.

```python
# Conceptual code analyzer
class CodebaseAnalyzer:
    def __init__(self):
        self.parsers = {
            ".py": PythonParser(),
            ".js": JavaScriptParser(),
            # Add more language parsers
        }
        self.llm_client = LLMClient()
        
    async def analyze_codebase(self, path: str) -> CodebaseAnalysis:
        # Get all code files
        files = self._get_code_files(path)
        
        # Parse each file
        parsed_files = []
        for file_path in files:
            extension = os.path.splitext(file_path)[1]
            if extension in self.parsers:
                parser = self.parsers[extension]
                parsed_file = await parser.parse(file_path)
                parsed_files.append(parsed_file)
        
        # Generate comprehensive documentation with AI assistance
        documentation = await self._generate_documentation(parsed_files)
        
        # Extract metrics
        metrics = self._extract_metrics(parsed_files)
        
        return CodebaseAnalysis(
            files=parsed_files,
            documentation=documentation,
            metrics=metrics
        )
```

### Knowledge Base Construction

The knowledge base system stores and indexes the generated documentation and analysis results for efficient retrieval.

**Key Components:**
- Vectorized document storage
- Semantic search capabilities
- Automatic updating and versioning
- Relevance scoring
- Query optimization
- Cross-reference linking

**Implementation Approach:**
Documentation will be chunked into semantic units, embedded using vector embeddings, and stored in a vector database for efficient similarity search. This allows the system to retrieve the most relevant documentation based on natural language queries.

```python
# Conceptual knowledge base
class KnowledgeBase:
    def __init__(self):
        self.embedding_model = EmbeddingModel()
        self.vector_store = VectorStore()
        
    async def add_documentation(self, documentation: List[Document]):
        # Create embeddings for each document
        for doc in documentation:
            # Create chunks
            chunks = self._create_chunks(doc.content)
            
            # Create embeddings
            embeddings = [await self.embedding_model.embed(chunk) for chunk in chunks]
            
            # Store in vector database
            await self.vector_store.add_embeddings(
                document_id=doc.id,
                embeddings=embeddings,
                metadata={
                    "title": doc.title,
                    "file_path": doc.file_path,
                    "type": doc.type
                }
            )
    
    async def query(self, query: str, top_k: int = 5) -> List[Document]:
        # Create query embedding
        query_embedding = await self.embedding_model.embed(query)
        
        # Search vector database
        results = await self.vector_store.search(
            embedding=query_embedding,
            top_k=top_k
        )
        
        # Retrieve full documents
        documents = [await self.vector_store.get_document(result.document_id) 
                    for result in results]
        
        return documents
```

### Metacognitive Layer

The metacognitive layer orchestrates the entire system, managing context, learning from interactions, and optimizing processes.

**Key Components:**
- Session context management
- Process orchestration and scheduling
- Adaptive learning from user interactions
- Performance monitoring and optimization
- Error pattern recognition
- Resource allocation
- User preference tracking

**Implementation Approach:**
This layer maintains an evolving model of the user's context, preferences, and working patterns. It uses this information to guide the AI's responses and optimize system behavior over time.

```python
# Conceptual metacognitive system
class MetacognitiveLayer:
    def __init__(self):
        self.context_manager = ContextManager()
        self.process_orchestrator = ProcessOrchestrator()
        self.learning_system = LearningSystem()
        
    async def process_user_request(self, request: UserRequest) -> Response:
        # Update context with new request
        self.context_manager.update_with_request(request)
        
        # Determine optimal processing strategy
        strategy = await self.process_orchestrator.determine_strategy(
            request=request,
            context=self.context_manager.get_current_context()
        )
        
        # Execute processing strategy
        response = await strategy.execute()
        
        # Learn from interaction
        await self.learning_system.record_interaction(
            request=request,
            response=response,
            context=self.context_manager.get_current_context()
        )
        
        # Update context with response
        self.context_manager.update_with_response(response)
        
        return response
```

The metacognitive layer includes several advanced capabilities:

1. **Contextual Understanding**: Maintains an evolving understanding of:
   - Current project structure and purpose
   - User's technical expertise level
   - Recent commands and their results
   - Error patterns and successful resolutions
   - Command preferences and usage patterns

2. **Workflow Optimization**:
   - Identifies repetitive patterns in user commands
   - Suggests workflow improvements and automation
   - Pre-emptively fetches likely-needed information
   - Prioritizes processing based on user history

3. **Adaptive Behavior**:
   - Adjusts verbosity based on user expertise
   - Tunes error handling detail level
   - Modifies visual presentation to user preferences
   - Evolves command suggestions based on acceptance rate

4. **Self-Improvement**:
   - Tracks success/failure of suggested solutions
   - Identifies knowledge gaps in the system
   - Prioritizes documentation enhancement areas
   - Builds personalized user support strategies

### Error Handling System

The error handling system detects, analyzes, and resolves errors encountered during command execution or code analysis.

**Key Components:**
- Error pattern recognition
- Multi-source error analysis
  - Local knowledge base
  - Web search integration
  - Error log history
- Contextual error explanation
- Solution generation and ranking
- Automatic error resolution with Git integration
- Resolution verification
- Error knowledge accumulation

**Implementation Approach:**
When an error occurs, the system analyzes it using both local knowledge and web searches, generates potential solutions ranked by likely effectiveness, and can automatically apply fixes without user intervention. All changes are tracked using Git for safety and auditability.

```python
# Conceptual error handler
class ErrorHandler:
    def __init__(self):
        self.knowledge_base = KnowledgeBase()
        self.web_searcher = WebSearcher()
        self.pattern_recognizer = ErrorPatternRecognizer()
        self.auto_resolver = AutoErrorResolver()
        
    async def analyze(self, command: str, return_code: int, stderr: str) -> ErrorAnalysis:
        # Recognize error pattern
        pattern = self.pattern_recognizer.recognize(stderr)
        
        # Search local knowledge base
        local_results = await self.knowledge_base.query(stderr, related_to="errors")
        
        # If insufficient local knowledge, search web
        if not self._has_sufficient_information(local_results):
            web_results = await self.web_searcher.search(stderr, pattern.search_query)
        else:
            web_results = []
            
        # Generate solutions
        solutions = await self._generate_solutions(
            command=command,
            error=stderr,
            pattern=pattern,
            local_results=local_results,
            web_results=web_results
        )
        
        # Attempt automatic resolution if possible
        fix_result = await self.auto_resolver.handle_error(
            command=command,
            return_code=return_code,
            error_text=stderr
        )
        
        if fix_result.success:
            return ErrorAnalysis(
                error=stderr,
                pattern=pattern,
                solutions=solutions,
                sources=local_results + web_results,
                auto_fix_result=fix_result
            )
        
        return ErrorAnalysis(
            error=stderr,
            pattern=pattern,
            solutions=solutions,
            sources=local_results + web_results
        )
```

The error handling system includes enhanced features:

1. **Intelligent Error Categorization**:
   - Distinguishes between syntax errors, runtime errors, and system limitations
   - Identifies permission issues, network problems, resource constraints
   - Recognizes tool-specific error patterns
   - Maps errors to common root causes

2. **Multi-level Solution Generation**:
   - Quick fixes for common errors
   - Comprehensive solutions for complex issues
   - Educational explanations for skill development
   - Alternative approaches when primary path is blocked

3. **Automatic Error Resolution**:
   - Identifies and automatically fixes common errors without user intervention
   - Creates Git branches to safely apply fixes
   - Tracks all changes in Git with descriptive commit messages
   - Reverts unsuccessful fixes automatically
   - Escalates to human only when automatic resolution fails

4. **Solution Verification**:
   - Simulates solutions before applying when possible
   - Monitors execution of fixes for success/failure
   - Provides rollback strategies for failed solutions
   - Learns from solution outcomes

5. **Error Knowledge Network**:
   - Builds relational map of error types and solutions
   - Identifies causal chains in complex errors
   - Connects errors to relevant documentation
   - Maintains project-specific error profiles

6. **Command-Line Integration**:
   - `zangalewa fix <command>` - Run a command with automatic error fixing
   - `zangalewa fix-script <script_file>` - Run a script with automatic error fixing for each command

### Visual Presentation Layer

The visual presentation layer enhances the terminal experience with rich, informative, and visually appealing elements.

**Key Components:**
- Theme and style management
- Progress visualization
- Data representation components
- Animated elements
- Layout management
- Color scheme handling
- Typography optimization

**Implementation Approach:**
Using Rich and Textual, the system will create a visually enhanced terminal experience that balances aesthetic appeal with information density and usability.

```python
# Conceptual visual renderer
class VisualRenderer:
    def __init__(self):
        self.console = rich.console.Console()
        self.theme = Theme.load("default")
        
    def render_progress(self, description: str, total: int, completed: int):
        # Create progress bar
        progress = Progress(
            SpinnerColumn(),
            TextColumn("[bold blue]{task.description}"),
            BarColumn(bar_width=None),
            TaskProgressColumn(),
            TimeRemainingColumn()
        )
        
        # Render progress
        with progress:
            task = progress.add_task(description, total=total)
            progress.update(task, completed=completed)
    
    def render_data(self, data: Any, data_type: str):
        # Select appropriate renderer based on data type
        if data_type == "table":
            return self._render_table(data)
        elif data_type == "code":
            return self._render_code(data)
        elif data_type == "tree":
            return self._render_tree(data)
        # Add more renderers
```

The visual presentation layer includes:

1. **Command Line Beautification**:
   - Custom-designed prompt with contextual elements
   - Syntax highlighting for commands and outputs
   - Semantic color coding for different information types
   - Unicode and emoji support for compact information display

2. **Advanced Progress Visualization**:
   - Multi-phase progress indicators for complex operations
   - Resource usage meters (CPU, memory, network)
   - Time estimation for long-running processes
   - Context-sensitive process details

3. **Data Visualization Components**:
   - Inline charts and graphs for numeric data
   - Structured format for tabular information
   - Collapsible tree views for hierarchical data
   - Diff highlighting for code and text changes

4. **Interactive Elements**:
   - Command suggestion panels
   - Expandable error details
   - In-terminal documentation viewers
   - Quick action menus for common operations

## Installation and Setup

Installation will be streamlined through Poetry:

```bash
pip install Zangalewa
```

### Configuration Setup

After installation, you'll need to configure Zangalewa with your API keys:

```bash
Zangalewa config setup
```

This interactive process will help you configure:
- LLM API keys (OpenAI, Claude)
- GitHub access tokens (if needed)
- Default project locations
- Visual theme preferences

### Environment Variables

Alternatively, you can set up the following environment variables:

```
HUGGINGFACE_API_KEY=<your-huggingface-api-key>
OPENAI_API_KEY=<your-openai-api-key>
ANTHROPIC_API_KEY=<your-anthropic-api-key>
ZANGALEWA_GITHUB_TOKEN=<your-github-token>
ZANGALEWA_PROJECT_DIR=<default-project-directory>
```

## Getting Started with HuggingFace API

Zangalewa now uses HuggingFace's API for language model interactions, allowing access to powerful models without downloading them locally. Here's how to get started:

### Prerequisites

- Git
- Python 3.10+ 
- HuggingFace account with API key

### Step 1: Clone the Repository

Open your terminal and run:

```bash
# Navigate to your desired installation folder
cd ~/Development

# Clone the Zangalewa repository
git clone https://github.com/fullscreen-triangle/zangalewa.git

# Navigate into the project directory
cd zangalewa
```

### Step 2: Install Zangalewa

```bash
# Create a virtual environment (recommended)
python -m venv .venv

# Activate the virtual environment
# For macOS/Linux:
source .venv/bin/activate
# For Windows:
# .venv\Scripts\activate

# Install dependencies and the package
pip install -e .
```

### Step 3: Set Up Your HuggingFace API Key

You'll need to get an API key from HuggingFace:

1. Create an account at [huggingface.co](https://huggingface.co) if you don't have one
2. Go to your profile settings > Access Tokens
3. Create a new token with 'read' scope
4. Set the API key as an environment variable:

```bash
# For macOS/Linux:
export HUGGINGFACE_API_KEY=your_api_key_here

# For Windows:
# set HUGGINGFACE_API_KEY=your_api_key_here
```

Alternatively, add it to your `.env` file in the project directory:

```
HUGGINGFACE_API_KEY=your_api_key_here
```

### Step 4: Check Your Configuration

Verify that your HuggingFace API key is properly configured:

```bash
zangalewa models status
```

You should see confirmation that your API key is configured and the models that will be used.

### Step 5: Launch Zangalewa

```bash
# Start Zangalewa
zangalewa
```

### Using Zangalewa with HuggingFace API

Once launched, Zangalewa will guide you through an interactive onboarding:

1. **Setting Up Your Workspace**
   ```
   > Where would you like to set up your main workspace?
   ```

2. **Using Basic Commands**
   ```
   > help
   ```
   This will display available commands and their descriptions.

3. **Adding Commercial API Keys (Optional)**
   You can add API keys for commercial models directly through the chat interface:
   ```
   > I'd like to add my OpenAI API key
   ```
   or
   ```
   > config add-key
   ```

4. **Working with Projects**
   ```
   > analyze project ./my-python-project
   ```

### Configured Models

Zangalewa uses different HuggingFace models for different purposes:

1. **General Purpose**: Mistral 7B Instruct
   - Used for chat, orchestration, and general tasks

2. **Code Generation**: CodeLlama 7B
   - Specialized for Python code generation and analysis

3. **Frontend Development**: DeepSeek Coder 6.7B
   - Optimized for React and frontend code generation

You can see which models are currently configured with:

```bash
zangalewa models info
```

### Troubleshooting

If you encounter issues with HuggingFace API integration:

1. Verify your API key is correct and properly set
2. Check your internet connection
3. Make sure your HuggingFace account has access to the required models
4. Check the Zangalewa logs for any connection issues with the HuggingFace API

## Usage Examples

### Project Setup

```bash
# Start the Zangalewa shell
$ Zangalewa

> I want to set up the bioinformatics tool FastQC for analyzing my sequencing data

Zangalewa: I'll help you set up FastQC. Let me handle the installation process for you.

[1/4] Searching for FastQC repository...
[2/4] Downloading FastQC v0.11.9...
[3/4] Setting up dependencies...
[4/4] Configuring environment...

✅ FastQC has been successfully installed and configured!

I've created documentation in ./docs/fastqc/ with:
- Function reference
- Common usage patterns
- Configuration guide

Would you like me to show you how to run a basic analysis?
```

### Error Resolution

```bash
> run fastqc -o ./results ./data/*.fastq

Zangalewa: Executing analysis with FastQC...

❌ Error detected: Permission denied: './results'

Analysis:
The 'results' directory either doesn't exist or you don't have write permission.

Recommended solutions:
1. Create the directory: mkdir -p ./results
2. Set proper permissions: chmod 755 ./results

Should I implement solution #1 for you?
```

### Workflow Optimization

```bash
> I need to process 300 FASTQ files, run quality control, alignment, and then variant calling

Zangalewa: Based on your system specs and the task description, I'll create an optimized pipeline.

📋 Proposed workflow:

1. Parallel FastQC on batches of 20 files (estimated time: 15 min)
2. Trimming with Trimmomatic using 8 threads (estimated time: 45 min)
3. Alignment with BWA-MEM using 12 threads (estimated time: 2.5 hours)
4. Variant calling with GATK HaplotypeCaller (estimated time: 3 hours)

Total estimate: ~7 hours

I've created a detailed pipeline in ./pipelines/variant_calling_pipeline.sh

This pipeline includes:
- Automatic error recovery
- Progress reporting
- Resume capability

Should I start execution now?
```

## Development Roadmap

### Phase 1: Core Functionality (8 weeks)
- Basic shell interface with LLM integration
- Command execution and error detection
- Simple codebase documentation
- Initial knowledge base implementation

### Phase 2: Advanced Features (12 weeks)
- Complete metacognitive layer
- Enhanced error resolution system
- Visual presentation improvements
- Workflow optimization

### Phase 3: Refinement and Expansion (8 weeks)
- Optimization for bioinformatics-specific tools
- Extended language support for code analysis
- Advanced learning capabilities
- User customization options

## Completed Improvements

All planned improvements for the Zangalewa project have been successfully implemented:

### Core Functionality
- Implemented actual LLM integration with support for multiple providers (OpenAI, Anthropic Claude)
- Added robust error handling, retries, and streaming response support
- Implemented caching for LLM responses to reduce API costs
- Created comprehensive prompt management system with templates
- Added function calling/tool use support and token usage tracking

### CLI Interface
- Completed AI processing for commands with rich text display
- Added command history navigation and tab completion
- Implemented help system and configuration wizard
- Created plugin system with custom aliases support

### Error Handling
- Expanded auto-fixable errors list with sophisticated pattern matching
- Added unit tests and feedback mechanism for error resolution
- Implemented visual diff viewer for code changes during error resolution
- Added support for multiple programming languages and error pattern tracking

### Knowledge Base
- Optimized vector storage for larger knowledge bases
- Implemented periodic reindexing and hierarchical knowledge organization
- Added backup/restore functionality and automatic knowledge updates
- Created import system for various sources and quality assessment

### Metacognitive Layer
- Implemented sophisticated relevance detection
- Added learning capabilities for improved suggestions
- Created context-aware command recommendations
- Implemented user expertise tracking, project-specific context, and workflow optimization

### Visual Presentation
- Enhanced styling with theme support and progress indicators
- Added data visualization components and collapsible sections
- Implemented syntax highlighting for multiple languages
- Added markdown rendering support in terminal

### Architecture and Security
- Implemented secure storage for API keys and sensitive data
- Added command sanitization and permissions system
- Created comprehensive logging and audit systems
- Implemented secure defaults and configuration validation

### Deployment and User Experience
- Created proper packaging with Docker containerization support
- Implemented plugin distribution system and automatic updates
- Added bioinformatics-specific features and workflows
- Created onboarding experience with progressive feature disclosure
- Implemented accessibility features and internationalization support

### Community and Documentation
- Created comprehensive API documentation with examples
- Added proper testing infrastructure including integration and property-based tests
- Implemented continuous integration and deployment
- Created contributor guides and community plugin system

## Contributing

We welcome contributions from the community! Please see [CONTRIBUTING.md](CONTRIBUTING.md) for guidelines on how to contribute to the project.

## License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## LLM Integration

Zangalewa requires a HuggingFace API key to function. Commercial APIs are optional supplements.

### HuggingFace API Requirement

Zangalewa uses the HuggingFace API which is **REQUIRED** for the package to function. The package uses three specialized models for different purposes:

- **Mistral 7B Instruct**: For general interaction and orchestration
- **CodeLlama 7B**: For Python code generation and analysis
- **DeepSeek Coder 6.7B**: For React and general code generation

#### Setting Up HuggingFace API

1. Create an account at [huggingface.co](https://huggingface.co) if you don't have one
2. Go to your profile settings > Access Tokens
3. Create a new token with 'read' scope
4. Set up the API key:

```bash
# Check your HuggingFace API configuration
zangalewa models status

# View information about models
zangalewa models info
```

> **IMPORTANT**: Zangalewa will not function without a valid HuggingFace API key. This key must be provided through environment variables or configuration files.

### Optional Commercial API Support

For enhanced capabilities, Zangalewa can optionally use commercial LLM providers:

- **OpenAI API** - Set your API key in the config or as an environment variable `OPENAI_API_KEY`
- **Anthropic API** - Set your API key in the config or as an environment variable `ANTHROPIC_API_KEY`

Commercial models are not required but can provide enhanced capabilities for complex tasks.

### Model Selection

Zangalewa automatically uses the appropriate model for different tasks:

- **Mistral 7B Instruct**: Used for general interaction, chat, and orchestration
- **CodeLlama 7B**: Used for Python code generation and analysis
- **DeepSeek Coder 6.7B**: Used for React and general code generation
- **Commercial APIs**: Used for complex reasoning tasks when available (optional)

You can configure model preferences in `config.yaml`:

```yaml
llm:
  primary_provider: "auto"  # Options: auto, openai, anthropic, huggingface
  openai:
    model: "gpt-4"
  anthropic:
    model: "claude-2"
  huggingface:
    temperature: 0.3
  huggingface_models:
    general: "mistralai/Mistral-7B-Instruct-v0.2"
    code: "codellama/CodeLlama-7b-hf"
    frontend: "deepseek-ai/deepseek-coder-6.7b-base"
```

Each model is optimized for specific tasks:

- **OpenAI/Anthropic**: Best for complex reasoning when API keys are available
- **CodeLlama**: Excels at Python code generation and analysis
- **DeepSeek Coder**: Specialized for ReactJS and general coding
- **Mistral**: Good all-around model for general text and code tasks
